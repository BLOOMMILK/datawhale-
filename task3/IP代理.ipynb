{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何应对IP被封的问题\n",
    "1. 修改请求头，模拟浏览器（而不是代码直接去访问）去访问\n",
    "2. 采用代理IP轮换\n",
    "3. 设置访问时间间隔\n",
    "\n",
    "# 如何获取代理IP地址\n",
    "- 从网站获取：https://www.xicidaili.com/\n",
    "- inspect -> 鼠标定位\n",
    "- 要获取的代理IP地址，属于class=\"odd\"标签的内容：代码如下，获取代理IP保存在proxy_ip_list列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-23a25f88f603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy_ip_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mproxy_ip_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_proxy_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-23a25f88f603>\u001b[0m in \u001b[0;36mget_proxy_ip\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxy_ip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'HTTP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'HTTPS'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{protocol}://{ip}:{port}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not callable"
     ]
    }
   ],
   "source": [
    "# 案例代码\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def open_proxy_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, timeout = 20)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return(r.text)\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "\n",
    "\n",
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    proxy_ips  = soup.select('.odd')#选择标签\n",
    "    for proxy_ip in proxy_ips:\n",
    "        ip = proxy_ip.select('td')[1].text\n",
    "        port = proxy_ip.select('td')[2].text\n",
    "        protocol = proxy_ip.select('td')[5].text\n",
    "        if protocol in ('HTTP','HTTPS'):\n",
    "            proxy_ip_list.append(f('{protocol}://{ip}:{port}'))\n",
    "    return proxy_ip_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    proxy_url = 'https://www.xicidaili.com/'\n",
    "    text = open_proxy_url(proxy_url)\n",
    "    proxy_ip_filename = 'proxy_ip.txt'\n",
    "    with open(proxy_ip_filename, 'w') as f:\n",
    "        f.write(text)\n",
    "    text = open(proxy_ip_filename, 'r').read()\n",
    "    proxy_ip_list = get_proxy_ip(text)\n",
    "    print(proxy_ip_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过bs4的find_all('tr')来获取所有IP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response,'htm.parser')\n",
    "    proxy_ips = soups.find(id = 'ip_list').fina_all('tr')\n",
    "    for proxy_ip in proxy_ips:\n",
    "        if len(proxy_ip.select('td')) >= 8:\n",
    "            ip = proxy_ip.select('td')[1].text\n",
    "            port = proxy_ip.select('td')[2].text\n",
    "            protocol = proxy_ip.select('td')[5].text\n",
    "            if protocol in ('HTTP','HTTPS','http','https'):\n",
    "                proxy_ip_list.append(f('{protocol}://{ip}:{port}'))\n",
    "    return proxy_ip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-19-4b0f32282610>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-4b0f32282610>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    return (r.text, r.status_code)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "def open_url_using_proxy(url, proxy):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxies = {}\n",
    "    if proxy.startswith('HTTPS'):\n",
    "        proxies['https'] = proxy\n",
    "    else:\n",
    "        proxies['http'] = proxy\n",
    "try:\n",
    "    r = requests.get(url, headers = headers, proxies = proxies, timeout = 10)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    return (r.text, r.status_code)\n",
    "except:\n",
    "    print('无法访问网页' + url)\n",
    "    return False\n",
    "url = 'http://www.baidu.com'\n",
    "text = open_url_using_proxy(url, proxy_ip_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确认代理IP地址有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    result = open_url_using_proxy(url, proxy)\n",
    "    VALID_PROXY = False\n",
    "    if result:\n",
    "        text, status_code = result\n",
    "        if status_code == 200:\n",
    "            print('有效代理IP: ' + proxy)\n",
    "        else:\n",
    "            print('无效代理IP: ' + proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确认网站title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    text, status_code = open_url_using_proxy(url, proxy)\n",
    "    VALID = False\n",
    "    if status_code == 200:\n",
    "        if r_title:\n",
    "            if r_title[0] == '<title>百度一下，你就知道</title>':\n",
    "                VALID = True\n",
    "    if VALID:\n",
    "        print('有效代理IP: ' + proxy)\n",
    "    else:\n",
    "        print('无效代理IP: ' + proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5e6b1ee8cbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mproxy_ip_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'proxy_ip.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy_ip_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mproxy_ip_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_proxy_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mproxy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mcheck_proxy_avaliability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-5e6b1ee8cbfc>\u001b[0m in \u001b[0;36mget_proxy_ip\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxy_ip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'HTTP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'HTTPS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'https'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{protocol}://{ip}:{port}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproxy_ip_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not callable"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def open_proxy_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, timeout = 10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "\n",
    "\n",
    "def get_proxy_ip(response):\n",
    "    proxy_ip_list = []\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    proxy_ips = soup.find(id = 'ip_list').find_all('tr')\n",
    "    for proxy_ip in proxy_ips:\n",
    "        if len(proxy_ip.select('td')) >=8:\n",
    "            ip = proxy_ip.select('td')[1].text\n",
    "            port = proxy_ip.select('td')[2].text\n",
    "            protocol = proxy_ip.select('td')[5].text\n",
    "            if protocol in ('HTTP','HTTPS','http','https'):\n",
    "                proxy_ip_list.append(f('{protocol}://{ip}:{port}'))\n",
    "    return proxy_ip_list\n",
    "\n",
    "\n",
    "def open_url_using_proxy(url, proxy):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxies = {}\n",
    "    if proxy.startswith(('HTTPS','https')):\n",
    "        proxies['https'] = proxy\n",
    "    else:\n",
    "        proxies['http'] = proxy\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers, proxies = proxies, timeout = 10)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return (r.text, r.status_code)\n",
    "    except:\n",
    "        print('无法访问网页' + url)\n",
    "        print('无效代理IP: ' + proxy)\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_proxy_avaliability(proxy):\n",
    "    url = 'http://www.baidu.com'\n",
    "    result = open_url_using_proxy(url, proxy)\n",
    "    VALID_PROXY = False\n",
    "    if result:\n",
    "        text, status_code = result\n",
    "        if status_code == 200:\n",
    "            r_title = re.findall('<title>.*</title>', text)\n",
    "            if r_title:\n",
    "                if r_title[0] == '<title>百度一下，你就知道</title>':\n",
    "                    VALID_PROXY = True\n",
    "        if VALID_PROXY:\n",
    "            check_ip_url = 'https://jsonip.com/'\n",
    "            try:\n",
    "                text, status_code = open_url_using_proxy(check_ip_url, proxy)\n",
    "            except:\n",
    "                return\n",
    "\n",
    "            print('有效代理IP: ' + proxy)\n",
    "            with open('valid_proxy_ip.txt','a') as f:\n",
    "                f.writelines(proxy)\n",
    "            try:\n",
    "                source_ip = json.loads(text).get('ip')\n",
    "                print(f('源IP地址为：{source_ip}'))\n",
    "                print('='*40)\n",
    "            except:\n",
    "                print('返回的非json,无法解析')\n",
    "                print(text)\n",
    "    else:\n",
    "        print('无效代理IP: ' + proxy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    proxy_url = 'https://www.xicidaili.com/'\n",
    "    proxy_ip_filename = 'proxy_ip.txt'\n",
    "    text = open(proxy_ip_filename, 'r').read()\n",
    "    proxy_ip_list = get_proxy_ip(text)\n",
    "    for proxy in proxy_ip_list:\n",
    "        check_proxy_avaliability(proxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
